
@misc{han2023chartllamamultimodalllmchart,
	title={ChartLlama: A Multimodal LLM for Chart Understanding and Generation}, 
	author={Yucheng Han and Chi Zhang and Xin Chen and Xu Yang and Zhibin Wang and Gang Yu and Bin Fu and Hanwang Zhang},
	year={2023},
	eprint={2311.16483},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2311.16483}, 
}

@misc{setty2024improvingretrievalragbased,
	title={Improving Retrieval for RAG based Question Answering Models on Financial Documents}, 
	author={Spurthi Setty and Harsh Thakkar and Alyssa Lee and Eden Chung and Natan Vidra},
	year={2024},
	eprint={2404.07221},
	archivePrefix={arXiv},
	primaryClass={cs.IR},
	url={https://arxiv.org/abs/2404.07221}, 
}

@article{toledo2023safety,
	title={Safety and efficacy of the two doses conjugated protein-based SOBERANA-02 COVID-19 vaccine and of a heterologous three-dose combination with SOBERANA-Plus: a double-blind, randomised, placebo-controlled phase 3 clinical trial},
	author={Toledo-Roman{\'\i}, Mar{\'\i}a Eugenia and Garc{\'\i}a-Carmenate, Mayra and Valenzuela-Silva, Carmen and Baldoqu{\'\i}n-Rodr{\'\i}guez, Waldemar and Mart{\'\i}nez-P{\'e}rez, Marisel and Rodr{\'\i}guez-Gonz{\'a}lez, Meiby and Paredes-Moreno, Beatriz and Mendoza-Hern{\'a}ndez, Ivis and Romero, Ra{\'u}l Gonz{\'a}lez-Mujica and Sam{\'o}n-Tabio, Oscar and others},
	journal={The Lancet Regional Health--Americas},
	volume={18},
	year={2023},
	publisher={Elsevier}
}

@misc{vaswani2023attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}

@book{lee2023climate,
	title={Climate change 2023: synthesis report. Contribution of working groups I, II and III to the sixth assessment report of the intergovernmental panel on climate change},
	author={Lee, Hoesung and Calvin, Katherine and Dasgupta, Dipak and Krinner, Gerhard and Mukherji, Aditi and Thorne, Peter and Trisos, Christopher and Romero, Jos{\'e} and Aldunce, Paulina and Barrett, Ko and others},
	year={2023},
	publisher={The Australian National University}
}

@book{hernandez_sampieri_metodologiinvestigacion_2014,
	address = {México D.F.},
	edition = {Sexta edición},
	title = {Metodología de la investigación},
	isbn = {978-1-4562-2396-0},
	language = {spa},
	publisher = {McGraw-Hill Education},
	author = {Hernández Sampieri, Roberto and Fernandez-Collado, Carlos F.},
	editor = {Baptista Lucio, Pilar},
	year = {2014},
	note = {OCLC: 952035471},
}

@misc{douze2024faisslibrary,
	title={The Faiss library}, 
	author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
	year={2024},
	eprint={2401.08281},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2401.08281}, 
}

@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	language = {en},
	number = {4},
	urldate = {2024-10-26},
	journal = {Communications of the ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
}

@article{delgado2022digital,
	title={Digital documents and Archival Legislation: the case of Mexico and Cuba},
	author={Delgado L{\'o}pez, Yorlis},
	journal={Investigaci{\'o}n bibliotecol{\'o}gica},
	volume={36},
	number={90},
	year={2022}
}

@article{parodi2017no,
	title={No solo existen palabras en los textos escritos: algunas teor{\'\i}as y modelos de comprensi{\'o}n de textos multimodales o multisemi{\'o}ticos},
	author={Parodi, Giovanni and Julio, Crist{\'o}bal},
	journal={Investigaciones sobre lectura},
	volume={8},
	pages={27--48},
	year={2017}
}

@misc{meta_llama_2024,
	title = {Llama 3.2: {Revolutionizing} edge {AI} and vision with open, customizable models},
	shorttitle = {Llama 3.2},
	url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
	language = {en},
	author = {Meta, Research},
	month = sep,
	year = {2024},
}

@misc{shi2024compressinglongcontextenhancing,
	title={Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation}, 
	author={Kaize Shi and Xueyao Sun and Qing Li and Guandong Xu},
	year={2024},
	eprint={2405.03085},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2405.03085}, 
}

@misc{chan_rq-rag_2024,
	title = {{RQ}-{RAG}: {Learning} to {Refine} {Queries} for {Retrieval} {Augmented} {Generation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{RQ}-{RAG}},
	url = {https://arxiv.org/abs/2404.00610},
	doi = {10.48550/ARXIV.2404.00610},
	abstract = {Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9{\textbackslash}\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{omrani2024hybrid,
	title={Hybrid Retrieval-Augmented Generation Approach for LLMs Query Response Enhancement},
	author={Omrani, Pouria and Hosseini, Alireza and Hooshanfar, Kiana and Ebrahimian, Zahra and Toosi, Ramin and Akhaee, Mohammad Ali},
	booktitle={2024 10th International Conference on Web Research (ICWR)},
	pages={22--26},
	year={2024},
	organization={IEEE}
}

@misc{dai2015documentembeddingparagraphvectors,
	title={Document Embedding with Paragraph Vectors}, 
	author={Andrew M. Dai and Christopher Olah and Quoc V. Le},
	year={2015},
	eprint={1507.07998},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1507.07998}, 
}

@article{zhang20241506,
	title={\# 1506 Uremic toxicity: gaining novel insights through AI-driven literature review},
	author={Zhang, Hanjie and Kotanko, Peter},
	journal={Nephrology Dialysis Transplantation},
	volume={39},
	number={Supplement\_1},
	pages={gfae069--0657},
	year={2024},
	publisher={Oxford University Press}
}

@article{Muludi2024,
	title = {Retrieval-Augmented Generation Approach: Document Question Answering using Large Language Model},
	journal = {International Journal of Advanced Computer Science and Applications},
	doi = {10.14569/IJACSA.2024.0150379},
	url = {http://dx.doi.org/10.14569/IJACSA.2024.0150379},
	year = {2024},
	publisher = {The Science and Information Organization},
	volume = {15},
	number = {3},
	author = {Kurnia Muludi and Kaira Milani Fitria and Joko Triloka and Sutedi}
}

@misc{huang2024surveyretrievalaugmentedtextgeneration,
	title={A Survey on Retrieval-Augmented Text Generation for Large Language Models}, 
	author={Yizheng Huang and Jimmy Huang},
	year={2024},
	eprint={2404.10981},
	archivePrefix={arXiv},
	primaryClass={cs.IR},
	url={https://arxiv.org/abs/2404.10981}, 
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/strange/Zotero/storage/97TWMTHB/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/XLSYI233/1301.html:text/html},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/QV8MT4W8/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/S52ZPEUW/2005.html:text/html},
}

@misc{bornea_telco-rag_2024,
	title = {Telco-{RAG}: {Navigating} the {Challenges} of {Retrieval}-{Augmented} {Language} {Models} for {Telecommunications}},
	shorttitle = {Telco-{RAG}},
	url = {http://arxiv.org/abs/2404.15939},
	abstract = {The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Bornea, Andrei-Laurentiu and Ayed, Fadhel and De Domenico, Antonio and Piovesan, Nicola and Maatouk, Ali},
	month = aug,
	year = {2024},
	note = {arXiv:2404.15939 [cs, eess]},
	keywords = {Computer Science - Information Retrieval, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/732IS69E/Bornea et al. - 2024 - Telco-RAG Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/EQUFXTUW/2404.html:text/html},
}

@misc{almeida_word_2023,
	title = {Word {Embeddings}: {A} {Survey}},
	shorttitle = {Word {Embeddings}},
	url = {http://arxiv.org/abs/1901.09069},
	abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Almeida, Felipe and Xexéo, Geraldo},
	month = may,
	year = {2023},
	note = {arXiv:1901.09069 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, A.1, I.2.7, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/AK5XVDF3/Almeida and Xexéo - 2023 - Word Embeddings A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/H934MCEC/1901.html:text/html},
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
	title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
	author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
	year={2024},
	eprint={2312.10997},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2312.10997}, 
}

@misc{zhao_meta-chunking_2024,
	title = {Meta-{Chunking}: {Learning} {Efficient} {Text} {Segmentation} via {Logical} {Perception}},
	shorttitle = {Meta-{Chunking}},
	url = {http://arxiv.org/abs/2410.12788},
	doi = {10.48550/arXiv.2410.12788},
	abstract = {Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances performance and speed, and precisely identifies the boundaries of text chunks by analyzing the characteristics of context perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines PPL Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8\% of the time. Furthermore, through the analysis of models of various scales and types, we observed that PPL Chunking exhibits notable flexibility and adaptability. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Zhao, Jihao and Ji, Zhiyuan and Feng, Yuchen and Qi, Pengnian and Niu, Simin and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2410.12788 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/strange/Zotero/storage/J2CQVS7Y/Zhao et al. - 2024 - Meta-Chunking Learning Efficient Text Segmentation via Logical Perception.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/EKE79328/2410.html:text/html},
}
