
@misc{han_chartllama_2023,
	title = {{ChartLlama}: {A} {Multimodal} {LLM} for {Chart} {Understanding} and {Generation}},
	shorttitle = {{ChartLlama}},
	url = {http://arxiv.org/abs/2311.16483},
	abstract = {Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Han, Yucheng and Zhang, Chi and Chen, Xin and Yang, Xu and Wang, Zhibin and Yu, Gang and Fu, Bin and Zhang, Hanwang},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16483 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/strange/Zotero/storage/ASHT47IJ/Han et al. - 2023 - ChartLlama A Multimodal LLM for Chart Understanding and Generation.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/W3MEXDUG/2311.html:text/html},
}

@misc{setty_improving_2024,
	title = {Improving {Retrieval} for {RAG} based {Question} {Answering} {Models} on {Financial} {Documents}},
	url = {http://arxiv.org/abs/2404.07221},
	abstract = {The effectiveness of Large Language Models (LLMs) in generating accurate responses relies heavily on the quality of input provided, particularly when employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by sourcing the most relevant text chunk(s) to base queries upon. Despite the significant advancements in LLMs' response quality in recent years, users may still encounter inaccuracies or irrelevant answers; these issues often stem from suboptimal text chunk retrieval by RAG rather than the inherent capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine the RAG process. This paper explores the existing constraints of RAG pipelines and introduces methodologies for enhancing text retrieval. It delves into strategies such as sophisticated chunking techniques, query expansion, the incorporation of metadata annotations, the application of re-ranking algorithms, and the fine-tuning of embedding algorithms. Implementing these approaches can substantially improve the retrieval quality, thereby elevating the overall performance and reliability of LLMs in processing and responding to queries.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Setty, Spurthi and Thakkar, Harsh and Lee, Alyssa and Chung, Eden and Vidra, Natan},
	month = aug,
	year = {2024},
	note = {arXiv:2404.07221 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Finance - General Finance},
	file = {Preprint PDF:/home/strange/Zotero/storage/AYZEJVLP/Setty et al. - 2024 - Improving Retrieval for RAG based Question Answering Models on Financial Documents.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/7Q26GDUX/2404.html:text/html},
}

@article{toledo2023safety,
	title={Safety and efficacy of the two doses conjugated protein-based SOBERANA-02 COVID-19 vaccine and of a heterologous three-dose combination with SOBERANA-Plus: a double-blind, randomised, placebo-controlled phase 3 clinical trial},
	author={Toledo-Roman{\'\i}, Mar{\'\i}a Eugenia and Garc{\'\i}a-Carmenate, Mayra and Valenzuela-Silva, Carmen and Baldoqu{\'\i}n-Rodr{\'\i}guez, Waldemar and Mart{\'\i}nez-P{\'e}rez, Marisel and Rodr{\'\i}guez-Gonz{\'a}lez, Meiby and Paredes-Moreno, Beatriz and Mendoza-Hern{\'a}ndez, Ivis and Romero, Ra{\'u}l Gonz{\'a}lez-Mujica and Sam{\'o}n-Tabio, Oscar and others},
	journal={The Lancet Regional Health--Americas},
	volume={18},
	year={2023},
	publisher={Elsevier}
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/strange/Zotero/storage/6BZ7SX9T/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@techreport{lee_ipcc_2023,
	title = {{IPCC}, 2023: {Climate} {Change} 2023: {Synthesis} {Report}. {Contribution} of {Working} {Groups} {I}, {II} and {III} to the {Sixth} {Assessment} {Report} of the {Intergovernmental} {Panel} on {Climate} {Change} [{Core} {Writing} {Team}, {H}. {Lee} and {J}. {Romero} (eds.)]. {IPCC}, {Geneva}, {Switzerland}.},
	shorttitle = {{IPCC}, 2023},
	url = {https://www.ipcc.ch/report/ar6/syr/},
	abstract = {The Synthesis Report (SYR) is a stand-alone synthesis of the most policy-relevant evidence from the scientific, technical, and socio-economic literature assessed in the Sixth Assessment Report (AR6) of the Intergovernmental Panel on Climate Change (IPCC). The SYR distils and integrates the main findings of the three reports of the Working Groups of the IPCC during the AR6, and the three AR6 Special Reports into a concise document. It consists of a Summary for Policymakers and a longer report.},
	urldate = {2024-11-17},
	institution = {Intergovernmental Panel on Climate Change (IPCC)},
	author = {Calvin, Katherine and Dasgupta, Dipak and Krinner, Gerhard and Mukherji, Aditi and Thorne, Peter W. and Trisos, Christopher and Romero, José and Aldunce, Paulina and Barrett, Ko and Blanco, Gabriel and Cheung, William W.L. and Connors, Sarah and Denton, Fatima and Diongue-Niang, Aïda and Dodman, David and Garschagen, Matthias and Geden, Oliver and Hayward, Bronwyn and Jones, Christopher and Jotzo, Frank and Krug, Thelma and Lasco, Rodel and Lee, Yune-Yi and Masson-Delmotte, Valérie and Meinshausen, Malte and Mintenbeck, Katja and Mokssit, Abdalah and Otto, Friederike E.L. and Pathak, Minal and Pirani, Anna and Poloczanska, Elvira and Pörtner, Hans-Otto and Revi, Aromar and Roberts, Debra C. and Roy, Joyashree and Ruane, Alex C. and Skea, Jim and Shukla, Priyadarshi R. and Slade, Raphael and Slangen, Aimée and Sokona, Youba and Sörensson, Anna A. and Tignor, Melinda and Van Vuuren, Detlef and Wei, Yi-Ming and Winkler, Harald and Zhai, Panmao and Zommers, Zinta and Hourcade, Jean-Charles and Johnson, Francis X. and Pachauri, Shonali and Simpson, Nicholas P. and Singh, Chandni and Thomas, Adelle and Totin, Edmond and Arias, Paola and Bustamante, Mercedes and Elgizouli, Ismail and Flato, Gregory and Howden, Mark and Méndez-Vallejo, Carlos and Pereira, Joy Jacqueline and Pichs-Madruga, Ramón and Rose, Steven K. and Saheb, Yamina and Sánchez Rodríguez, Roberto and Ürge-Vorsatz, Diana and Xiao, Cunde and Yassaa, Noureddine and Alegría, Andrés and Armour, Kyle and Bednar-Friedl, Birgit and Blok, Kornelis and Cissé, Guéladio and Dentener, Frank and Eriksen, Siri and Fischer, Erich and Garner, Gregory and Guivarch, Céline and Haasnoot, Marjolijn and Hansen, Gerrit and Hauser, Mathias and Hawkins, Ed and Hermans, Tim and Kopp, Robert and Leprince-Ringuet, Noëmie and Lewis, Jared and Ley, Debora and Ludden, Chloé and Niamir, Leila and Nicholls, Zebedee and Some, Shreya and Szopa, Sophie and Trewin, Blair and Van Der Wijst, Kaj-Ivar and Winter, Gundula and Witting, Maximilian and Birt, Arlene and Ha, Meeyoung and Romero, José and Kim, Jinmi and Haites, Erik F. and Jung, Yonghun and Stavins, Robert and Birt, Arlene and Ha, Meeyoung and Orendain, Dan Jezreel A. and Ignon, Lance and Park, Semin and Park, Youngin and Reisinger, Andy and Cammaramo, Diego and Fischlin, Andreas and Fuglestvedt, Jan S. and Hansen, Gerrit and Ludden, Chloé and Masson-Delmotte, Valérie and Matthews, J.B. Robin and Mintenbeck, Katja and Pirani, Anna and Poloczanska, Elvira and Leprince-Ringuet, Noëmie and Péan, Clotilde},
	collaborator = {Lee, Hoesung},
	month = jul,
	year = {2023},
	doi = {10.59327/IPCC/AR6-9789291691647},
	note = {Edition: First},
	file = {Submitted Version:/home/strange/Zotero/storage/EJ5AIQSX/Calvin et al. - 2023 - IPCC, 2023 Climate Change 2023 Synthesis Report. Contribution of Working Groups I, II and III to t.pdf:application/pdf},
}

@book{hernandez_sampieri_metodologiinvestigacion_2014,
	address = {México D.F.},
	edition = {Sexta edición},
	title = {Metodología de la investigación},
	isbn = {978-1-4562-2396-0},
	language = {spa},
	publisher = {McGraw-Hill Education},
	author = {Hernández Sampieri, Roberto and Fernandez-Collado, Carlos F.},
	editor = {Baptista Lucio, Pilar},
	year = {2014},
	note = {OCLC: 952035471},
}

@misc{douze_faiss_2024,
	title = {The {Faiss} library},
	url = {http://arxiv.org/abs/2401.08281},
	abstract = {Vector databases typically manage large collections of embedding vectors. Currently, AI applications are growing rapidly, and so is the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper describes the trade-off space of vector search and the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Douze, Matthijs and Guzhva, Alexandr and Deng, Chengqi and Johnson, Jeff and Szilvasy, Gergely and Mazaré, Pierre-Emmanuel and Lomeli, Maria and Hosseini, Lucas and Jégou, Hervé},
	month = sep,
	year = {2024},
	note = {arXiv:2401.08281 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {Preprint PDF:/home/strange/Zotero/storage/E585FE9Q/Douze et al. - 2024 - The Faiss library.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/YDQB3JCL/2401.html:text/html},
}

@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	language = {en},
	number = {4},
	urldate = {2024-10-26},
	journal = {Communications of the ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
}

@article{delgado_lopez_documentos_2022,
	title = {Documentos digitales y legislación archivística: el caso de {México} y {Cuba}},
	volume = {36},
	issn = {2448-8321, 0187-358X},
	shorttitle = {Documentos digitales y legislación archivística},
	url = {http://rev-ib.unam.mx/ib/index.php/ib/article/view/58496},
	doi = {10.22201/iibi.24488321xe.2022.90.58496},
	abstract = {El patrimonio documental de una nación tiene un fuerte componente de los documentos digitales que se generan hoy en la administración pública. La gestión, el tratamiento y el acceso a esta especificidad documental son objeto de regulación y protección por el Derecho. Las legislaciones archivísticas de América Latina tienen en estos soportes un reto ineludible. México (2018) y Cuba (2020) son los cuerpos legales más contemporáneos y ya evocan estos registros. Con esta premisa, el objetivo de este artículo es analizar comparativamente las regulaciones jurídicas de los documentos digitales, a partir la legislación archivística de México y Cuba, con el fin de obtener apreciaciones conclusivas que sirvan de norma para el territorio latinoamericano. La metodología y las técnicas empleadas fueron las inherentes al campo teórico-jurídico, el derecho comparado, analítico-deductivo y la técnica de análisis de documentos jurídicos. Se obtuvo una comparación valorativa de ambos corpus en cuanto a este objeto de protección y se infieren recomendaciones en este sentido para Latinoamérica. Se concluyó que los documentos digitales son parte indisoluble del patrimonio documental; la posición mexicana y cubana de protección legal del documento digital y su archivo, en espíritu normativo, es no contemplar el soporte.},
	number = {90},
	urldate = {2024-10-26},
	journal = {Investigación Bibliotecológica: archivonomía, bibliotecología e información},
	author = {Delgado López, Yorlis},
	month = feb,
	year = {2022},
	pages = {119},
	file = {Full Text PDF:/home/strange/Zotero/storage/WZJYIIZM/Delgado López - 2022 - Documentos digitales y legislación archivística el caso de México y Cuba.pdf:application/pdf},
}

@article{giovanni_no_2017,
	title = {No solo existen palabras en los textos escritos: algunas teorías y modelos de comprensión de textos multimodales o multisemióticos},
	url = {https://api.semanticscholar.org/CorpusID:193859211},
	author = {Giovanni, Parodi Sweis and Cristobal, Julio},
	year = {2017},
}

@misc{meta_llama_2024,
	title = {Llama 3.2: {Revolutionizing} edge {AI} and vision with open, customizable models},
	shorttitle = {Llama 3.2},
	url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
	language = {en},
	author = {Meta, Research},
	month = sep,
	year = {2024},
}

@misc{shi_compressing_2024,
	title = {Compressing {Long} {Context} for {Enhancing} {RAG} with {AMR}-based {Concept} {Distillation}},
	url = {http://arxiv.org/abs/2405.03085},
	abstract = {Large Language Models (LLMs) have made significant strides in information acquisition. However, their overreliance on potentially flawed parametric knowledge leads to hallucinations and inaccuracies, particularly when handling long-tail, domain-specific queries. Retrieval Augmented Generation (RAG) addresses this limitation by incorporating external, non-parametric knowledge. Nevertheless, the retrieved long-context documents often contain noisy, irrelevant information alongside vital knowledge, negatively diluting LLMs' attention. Inspired by the supportive role of essential concepts in individuals' reading comprehension, we propose a novel concept-based RAG framework with the Abstract Meaning Representation (AMR)-based concept distillation algorithm. The proposed algorithm compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features. The concepts explicitly constrain LLMs to focus solely on vital information in the inference process. We conduct extensive experiments on open-domain question-answering datasets to empirically evaluate the proposed method's effectiveness. The results indicate that the concept-based RAG framework outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs. This emphasizes the distilled concepts are informative for augmenting the RAG process by filtering out interference information. To the best of our knowledge, this is the first work introducing AMR to enhance the RAG, presenting a potential solution to augment inference performance with semantic-based context compression.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Shi, Kaize and Sun, Xueyao and Li, Qing and Xu, Guandong},
	month = may,
	year = {2024},
	note = {arXiv:2405.03085 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/strange/Zotero/storage/U6JQVHTC/Shi et al. - 2024 - Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/WKCKK5JL/2405.html:text/html},
}

@misc{chan_rq-rag_2024,
	title = {{RQ}-{RAG}: {Learning} to {Refine} {Queries} for {Retrieval} {Augmented} {Generation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{RQ}-{RAG}},
	url = {https://arxiv.org/abs/2404.00610},
	doi = {10.48550/ARXIV.2404.00610},
	abstract = {Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9{\textbackslash}\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
	year = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{omrani_hybrid_2024,
	address = {Tehran, Iran, Islamic Republic of},
	title = {Hybrid {Retrieval}-{Augmented} {Generation} {Approach} for {LLMs} {Query} {Response} {Enhancement}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-9498-6},
	url = {https://ieeexplore.ieee.org/document/10533345/},
	doi = {10.1109/ICWR61162.2024.10533345},
	urldate = {2024-10-23},
	booktitle = {2024 10th {International} {Conference} on {Web} {Research} ({ICWR})},
	publisher = {IEEE},
	author = {Omrani, Pouria and Hosseini, Alireza and Hooshanfar, Kiana and Ebrahimian, Zahra and Toosi, Ramin and Ali Akhaee, Mohammad},
	month = apr,
	year = {2024},
	pages = {22--26},
}

@misc{dai_document_2015,
	title = {Document {Embedding} with {Paragraph} {Vectors}},
	url = {http://arxiv.org/abs/1507.07998},
	abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
	month = jul,
	year = {2015},
	note = {arXiv:1507.07998 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/strange/Zotero/storage/9PJV5F4X/Dai et al. - 2015 - Document Embedding with Paragraph Vectors.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/2NW45SM7/1507.html:text/html},
}

@article{zhang_1506_2024,
	title = {\#1506 {Uremic} toxicity: gaining novel insights through {AI}-driven literature review},
	volume = {39},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0931-0509, 1460-2385},
	shorttitle = {\#1506 {Uremic} toxicity},
	url = {https://academic.oup.com/ndt/article/doi/10.1093/ndt/gfae069.657/7678629},
	doi = {10.1093/ndt/gfae069.657},
	abstract = {Abstract
            
              Background and Aims
              The rapidly growing scientific literature poses a significant challenge for researchers seeking to distill key insights. We utilized Retrieval-Augmented Generation (RAG), a novel AI-driven approach, to efficiently process and extract meaningful information from published literature on uremic toxins. RAG is a general AI framework for improving the quality of responses generated by Large Language Models (LLMs) by supplementing the LLM's internal representation of information with curated expert knowledge.
            
            
              Method
              First, we collected on PubMed all abstracts related to the topic of ?uremic toxins? through Metapub, a Python library designed to facilitate fetching metadata from PubMed. Second, we set up a RAG system that comprises 2 steps. In a retrieval step, the questions on topic (?uremic toxins?) and the documents (=all collected abstracts and manuscripts) are encoded into vectors (i.e., high-dimensional numerical representations). Similarity measures are used to find the best matches between documents and the questions on topic. Second, in the augmented generation step, the LLM (e.g., ChatGPT) uses these best matches of documents to generate a coherent and informed response.
            
            
              Results
              We collected 3497 abstracts from the PubMed and 191 expert-curated publications in PDF format related to the topic ?uremic toxin?. These 191 publications were broken down to 5756 documents, each with a manageable size of text. The final vector database comprised 9253 vectors. Using RAG, we requested responses from the LLM on multiple questions related to ?uremic toxins?. Some examples are shown in Table 1. The first and second responses given by the LLM are reasonable. However, the third answer shows the phenomenon of ?hallucination??where models generate plausible and convincingly sounding yet factually incorrect information.
            
            
              Conclusion
              The use of RAG improves the capability of LLMs to answer questions by leveraging the information contained within curated abstracts and publications. Despite the improvements with RAG, the phenomenon of ?hallucination? persists. A concerning feature of hallucinations is their eloquent and convincing language. For the time being, LLM output?even when improved with RAG?requires scrutiny and human verification.},
	language = {en},
	number = {Supplement\_1},
	urldate = {2024-10-23},
	journal = {Nephrology Dialysis Transplantation},
	author = {Zhang, Hanjie and Kotanko, Peter},
	month = may,
	year = {2024},
	pages = {gfae069--0657--1506},
	file = {Full Text:/home/strange/Zotero/storage/FKQBZX8E/Zhang and Kotanko - 2024 - #1506 Uremic toxicity gaining novel insights through AI-driven literature review.pdf:application/pdf},
}

@article{muludi_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} {Approach}: {Document} {Question} {Answering} using {Large} {Language} {Model}},
	volume = {15},
	issn = {21565570, 2158107X},
	shorttitle = {Retrieval-{Augmented} {Generation} {Approach}},
	url = {http://thesai.org/Publications/ViewPaper?Volume=15&Issue=3&Code=IJACSA&SerialNo=79},
	doi = {10.14569/IJACSA.2024.0150379},
	language = {en},
	number = {3},
	urldate = {2024-10-23},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Muludi, Kurnia and Fitria, Kaira Milani and Triloka, Joko and -, Sutedi},
	year = {2024},
	file = {Full Text:/home/strange/Zotero/storage/YDSBKI78/Muludi et al. - 2024 - Retrieval-Augmented Generation Approach Document Question Answering using Large Language Model.pdf:application/pdf},
}

@misc{huang_survey_2024,
	title = {A {Survey} on {Retrieval}-{Augmented} {Text} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2404.10981},
	abstract = {Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Huang, Yizheng and Huang, Jimmy},
	month = aug,
	year = {2024},
	note = {arXiv:2404.10981 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/strange/Zotero/storage/Q9SCPTXB/Huang and Huang - 2024 - A Survey on Retrieval-Augmented Text Generation for Large Language Models.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/5926966N/2404.html:text/html},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/strange/Zotero/storage/97TWMTHB/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/XLSYI233/1301.html:text/html},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/QV8MT4W8/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/S52ZPEUW/2005.html:text/html},
}

@misc{bornea_telco-rag_2024,
	title = {Telco-{RAG}: {Navigating} the {Challenges} of {Retrieval}-{Augmented} {Language} {Models} for {Telecommunications}},
	shorttitle = {Telco-{RAG}},
	url = {http://arxiv.org/abs/2404.15939},
	abstract = {The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Bornea, Andrei-Laurentiu and Ayed, Fadhel and De Domenico, Antonio and Piovesan, Nicola and Maatouk, Ali},
	month = aug,
	year = {2024},
	note = {arXiv:2404.15939 [cs, eess]},
	keywords = {Computer Science - Information Retrieval, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/732IS69E/Bornea et al. - 2024 - Telco-RAG Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/EQUFXTUW/2404.html:text/html},
}

@misc{almeida_word_2023,
	title = {Word {Embeddings}: {A} {Survey}},
	shorttitle = {Word {Embeddings}},
	url = {http://arxiv.org/abs/1901.09069},
	abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Almeida, Felipe and Xexéo, Geraldo},
	month = may,
	year = {2023},
	note = {arXiv:1901.09069 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, A.1, I.2.7, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/AK5XVDF3/Almeida and Xexéo - 2023 - Word Embeddings A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/H934MCEC/1901.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/strange/Zotero/storage/4WS776VU/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/strange/Zotero/storage/9KLAH5HJ/2312.html:text/html},
}

@misc{zhao_meta-chunking_2024,
	title = {Meta-{Chunking}: {Learning} {Efficient} {Text} {Segmentation} via {Logical} {Perception}},
	shorttitle = {Meta-{Chunking}},
	url = {http://arxiv.org/abs/2410.12788},
	doi = {10.48550/arXiv.2410.12788},
	abstract = {Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances performance and speed, and precisely identifies the boundaries of text chunks by analyzing the characteristics of context perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines PPL Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8\% of the time. Furthermore, through the analysis of models of various scales and types, we observed that PPL Chunking exhibits notable flexibility and adaptability. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Zhao, Jihao and Ji, Zhiyuan and Feng, Yuchen and Qi, Pengnian and Niu, Simin and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2410.12788 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/strange/Zotero/storage/J2CQVS7Y/Zhao et al. - 2024 - Meta-Chunking Learning Efficient Text Segmentation via Logical Perception.pdf:application/pdf;Snapshot:/home/strange/Zotero/storage/EKE79328/2410.html:text/html},
}
